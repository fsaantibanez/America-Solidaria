{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0HQ_Iad83rX",
        "outputId": "3c19f7f5-3dc2-4118-c40d-efedea10710c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=0539661da01cf55064876d425269a672a146c2076c3fe05ed67ce4536d72bcca\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBF1YZZK80jY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "from functools import reduce\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import hashlib\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import chardet\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "2MPKmH3d-FQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones globales**"
      ],
      "metadata": {
        "id": "8Cr6KOmkC9im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_save_credentials():\n",
        "    \"\"\"\n",
        "    Descarga un archivo desde Google Cloud Storage y lo guarda localmente.\n",
        "\n",
        "    Returns:\n",
        "    - str: La ruta del archivo guardado localmente.\n",
        "    \"\"\"\n",
        "    # Variables\n",
        "    bucket_name = 'credentials-gs'\n",
        "    blob_name = 'credentials.json'\n",
        "    credentials_temp_path = '/tmp/google_credentials.json'\n",
        "\n",
        "    # Crea un cliente de Google Cloud Storage\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Obtén el bucket y el blob\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    # Descarga el contenido del archivo\n",
        "    credentials_content = blob.download_as_text()\n",
        "\n",
        "    # Guarda el contenido en el archivo local\n",
        "    with open(credentials_temp_path, 'w') as local_file:\n",
        "        local_file.write(credentials_content)\n",
        "\n",
        "    # Retorna la ruta del archivo guardado localmente\n",
        "    return credentials_temp_path"
      ],
      "metadata": {
        "id": "5yYXn-mw-DhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_id(num_records):\n",
        "  \"\"\"Genera una tabla con `num_records` registros aleatorios con valores correlativos.\"\"\"\n",
        "  table = []\n",
        "  epoch_value = int(time.time())  # Obtener el valor de epoch\n",
        "\n",
        "  for i in range(num_records):\n",
        "      # Concatenar el epoch con el UUID para agregar más entropía\n",
        "      combined_data = f\"{epoch_value}-{uuid.uuid4()}\"\n",
        "\n",
        "      # Generar un hash (por ejemplo, MD5) del dato combinado para obtener un UUID más corto\n",
        "      hash_uuid = hashlib.md5(combined_data.encode()).hexdigest()\n",
        "\n",
        "      table.append({\n",
        "          \"donation_id\": hash_uuid[:20]  # Tomar los primeros 20 caracteres (ajusta según sea necesario)\n",
        "      })\n",
        "\n",
        "  df = pd.DataFrame(table)\n",
        "  return df"
      ],
      "metadata": {
        "id": "pZjsEcTj-Opf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gcs_bucket(bucket_name, credentials_path):\n",
        "    # Inicializar el cliente de almacenamiento con credenciales desde el archivo\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "        credentials_path,\n",
        "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
        "    )\n",
        "\n",
        "    # Crear el cliente de almacenamiento\n",
        "    client = storage.Client(credentials=credentials)\n",
        "\n",
        "    # Obtener el bucket\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "    return bucket"
      ],
      "metadata": {
        "id": "vbSFdOTN9we8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def files_list_from_folder(bucket_name, carpeta):\n",
        "    # Inicializar el cliente de almacenamiento\n",
        "    client = storage.Client()\n",
        "\n",
        "    # Obtener el bucket\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "    # Listar archivos en la subcarpeta especificada\n",
        "    blobs = bucket.list_blobs(prefix=carpeta)\n",
        "\n",
        "    # Filtrar archivos excluyendo la carpeta 'PAT/'\n",
        "    archivos = [blob.name for blob in blobs if blob.name != carpeta]\n",
        "\n",
        "    # Imprimir nombres de archivos en la subcarpeta (opcional)\n",
        "    # for archivo in archivos:\n",
        "    #     print(archivo)\n",
        "\n",
        "    return archivos"
      ],
      "metadata": {
        "id": "mMiimmr1973t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_row_count(project_id, dataset_id, table_id):\n",
        "  try:\n",
        "      # Create a BigQuery client\n",
        "      client = bigquery.Client(project=project_id)\n",
        "\n",
        "      # Construir la consulta COUNT(*)\n",
        "      count_sql_script = f\"SELECT COUNT(*) as count_result FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
        "\n",
        "      # Ejecutar la consulta\n",
        "      query_job = client.query(count_sql_script)\n",
        "\n",
        "      # Esperar a que la consulta se complete\n",
        "      result = query_job.result()\n",
        "\n",
        "      # Acceder al valor de COUNT(*)\n",
        "      count_result = next(result).get('count_result')\n",
        "\n",
        "      return count_result\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error executing COUNT(*) query: {str(e)}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "J83e_uoYB--e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_df_to_bq(dataframe, project_id, dataset_id, table_id):\n",
        "\n",
        "  client = bigquery.Client(project=project_id)\n",
        "\n",
        "  # Define the destination table\n",
        "  table_ref = client.dataset(dataset_id).table(table_id)\n",
        "  #dataframe[['external_id','created_on']] = dataframe[['external_id','created_on']].astype(str)\n",
        "  #if format is not None and format in dataframe.columns:\n",
        "  #        dataframe[format] = dataframe[format].astype(str)\n",
        "\n",
        "  # Create the job configuration\n",
        "  job_config = bigquery.LoadJobConfig()\n",
        "  job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # Replace the table\n",
        "\n",
        "  # Load the DataFrame into BigQuery\n",
        "  client.load_table_from_dataframe(dataframe, table_ref, job_config=job_config).result()"
      ],
      "metadata": {
        "id": "Rnr32rPjCNqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bq_to_df(query):\n",
        "  try:\n",
        "      # Crear un cliente de BigQuery\n",
        "      client = bigquery.Client(project=project_id)\n",
        "\n",
        "      # Ejecutar la consulta SQL para realizar el upsert\n",
        "      query_job = client.query(f\"{query}\")\n",
        "\n",
        "      # Esperar a que se complete la consulta y almacenar los resultados en un DataFrame\n",
        "      result_dataframe = query_job.to_dataframe()\n",
        "\n",
        "      print(\"Operación completada en BigQuery\")\n",
        "\n",
        "      return result_dataframe\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "mIEA0Dd4CCFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_sql_script(project_id, upsert_sql_script):\n",
        "  # Create a BigQuery client\n",
        "  client = bigquery.Client(project=project_id)\n",
        "\n",
        "  # Execute the SQL script to perform the upsert\n",
        "  query_job = client.query(f\"{upsert_sql_script}\")\n",
        "\n",
        "  query_job.result()\n",
        "\n",
        "  # Wait for the query to complete\n",
        "\n",
        "  print(\"Operation completed in BigQuery\")"
      ],
      "metadata": {
        "id": "l0O3cPoYCH8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Seteo de variables de entorno**"
      ],
      "metadata": {
        "id": "kCkq_q9FCsG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"ProcesamientoGCS\").getOrCreate()\n",
        "\n",
        "bucket_name = \"your-bucket-name\"\n",
        "project_id = 'your-project-id'\n",
        "dataset_id = 'your-dataset-id'\n",
        "\n",
        "credentials_path = download_and_save_credentials()\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path"
      ],
      "metadata": {
        "id": "Uf-m7gaL92JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Backup BLOB 1**"
      ],
      "metadata": {
        "id": "EIi_co-IBOaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob1_prep = \"\"\"\n",
        "CREATE OR REPLACE TABLE `your-project_id.your-dataset_id.your_table-id` AS\n",
        "WITH PrepStep1 AS (\n",
        "  SELECT donation_id,\n",
        "       data,\n",
        "       CAST(LEFT(SUBSTR(data,4,22),LENGTH(SUBSTR(data,4,22))-1) AS INT64) id,\n",
        "       CONCAT(SUBSTR(data,26,4),'-',SUBSTR(data,30,2),'-' ,SUBSTR(data,32,2)) transaction_date,\n",
        "       CONCAT(SUBSTR(data,26,4),'-',SUBSTR(data,30,2),'-' ,SUBSTR(data,32,2)) close_date,\n",
        "       CAST(SUBSTR(data,34,12) AS INT64) transaction_amount,\n",
        "       CAST(SUBSTR(data,48,3) AS INT64) transaction_bank_code,\n",
        "FROM `your-project_id.your-dataset_id.your_table-id`\n",
        "),PrepStep2 AS (\n",
        " SELECT donation_id,\n",
        "        data,\n",
        "        id,\n",
        "        transaction_amount,\n",
        "        transaction_date,\n",
        "        close_date,\n",
        "        transaction_bank_code,\n",
        "        IF(data NOT LIKE '%APROBADO%',3,1) transaction_status,\n",
        "        REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(data,SUBSTR(data,4,22),''),SUBSTR(data,26,8),''),SUBSTR(data,48,3),''),SUBSTR(data,34,12),'') norm\n",
        "FROM PrepStep1\n",
        ")SELECT donation_id,\n",
        "        id,\n",
        "        transaction_amount,\n",
        "        NULL account_number,\n",
        "        2 plan_id,\n",
        "        CONCAT(\"XXXX \",CASE WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 1 THEN 'Enero'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 2 THEN 'Febrero'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 3 THEN 'Marzo'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 4 THEN 'Abril'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 5 THEN 'Mayo'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 6 THEN 'Junio'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 7 THEN 'Julio'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 8 THEN 'Agosto'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 9 THEN 'Septiembre'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 10 THEN 'Octubre'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 11 THEN 'Noviembre'\n",
        "                                WHEN CAST(SUBSTR(close_date,6,2) AS INT64) = 12 THEN 'Diciembre' END,\" \",SUBSTR(close_date,1,4)\n",
        "                  )  transaction_source,\n",
        "        1 transaction_source_code,\n",
        "        transaction_date,\n",
        "        close_date,\n",
        "        transaction_status,\n",
        "        CASE WHEN norm LIKE '%R01%' AND data NOT LIKE '%APROBADO%' THEN 'Fondos Insuficientes'\n",
        "             WHEN norm LIKE '%R02%' AND data NOT LIKE '%APROBADO%' THEN 'Cuenta Cerrada / No Operativa'\n",
        "             WHEN norm LIKE '%R04%' AND data NOT LIKE '%APROBADO%' THEN 'Cuenta No Existe'\n",
        "             WHEN norm LIKE '%R07%' AND data NOT LIKE '%APROBADO%' THEN 'Autorizacion Revocada Por El C'\n",
        "             WHEN norm LIKE '%R08%' AND data NOT LIKE '%APROBADO%' THEN 'Orden De No Cargo'\n",
        "             WHEN norm LIKE '%R10%' AND data NOT LIKE '%APROBADO%' THEN 'Mandato Inexistente'\n",
        "             WHEN norm LIKE '%R82%' AND data NOT LIKE '%APROBADO%' THEN 'Monto De La Operación Excede M'\n",
        "             WHEN norm LIKE '%P01%' AND data NOT LIKE '%APROBADO%' THEN 'No Existe Identificador Del Cl'\n",
        "             WHEN norm LIKE '%R93%' AND data NOT LIKE '%APROBADO%' THEN 'Eliminado Por El Cliente' ELSE NULL END transaction_details,\n",
        "        CAST(NULL AS INT64) transaction_bank_code,\n",
        "        CAST(NULL AS STRING) transaction_payment_form,\n",
        "        true individual_donor\n",
        "FROM PrepStep2\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VUVYKQcYFdSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blob1(bucket_name, credentials_path, file_list, folder_prefix):\n",
        "    # Inicializar SparkSession\n",
        "    spark = SparkSession.builder.appName(\"ProcesamientoGCS\").getOrCreate()\n",
        "\n",
        "    # Obtener el bucket\n",
        "    bucket = get_gcs_bucket(bucket_name, credentials_path)\n",
        "\n",
        "    # Fijar el prefijo del directorio\n",
        "    folder_prefix = folder_prefix\n",
        "\n",
        "    # Obtener iterador de blobs en el directorio 'folder_prefix'\n",
        "    blobs = bucket.list_blobs(prefix=folder_prefix)\n",
        "\n",
        "    # Filtrar solo los blobs correspondientes a archivos, no a subdirectorios\n",
        "    archivos_en_blob1 = [blob for blob in blobs if '/' not in blob.name[len(folder_prefix):] and blob.name.startswith(folder_prefix + 'your-file-pattern')]\n",
        "\n",
        "    # Verificar si hay archivos en el storage que no estén en la lista\n",
        "    archivos_a_procesar = [blob for blob in archivos_en_blob1 if blob.name[len(folder_prefix):] not in file_list]\n",
        "\n",
        "    # Si no hay archivos para procesar, devuelve None\n",
        "    if not archivos_a_procesar:\n",
        "        return None\n",
        "\n",
        "    # Procesar cada archivo en paralelo\n",
        "    dataframes = []\n",
        "\n",
        "    for blob in archivos_a_procesar:\n",
        "        nombre_archivo = blob.name[len(folder_prefix):]\n",
        "\n",
        "        # Imprimir el nombre del archivo que se está procesando\n",
        "        print(f\"Procesando archivo: {nombre_archivo}\")\n",
        "\n",
        "        # Procesar el archivo y obtener el DataFrame\n",
        "        df = procesar_archivo_google_storage(spark, bucket_name, blob.name, credentials_path)\n",
        "\n",
        "        # Agregar una columna con el nombre del archivo\n",
        "        df = df.withColumn(\"filename\", lit(nombre_archivo))\n",
        "\n",
        "        dataframes.append(df)\n",
        "\n",
        "    # Unir los DataFrames en uno solo\n",
        "    combined_df = reduce(lambda df1, df2: df1.union(df2), dataframes)\n",
        "\n",
        "    return combined_df"
      ],
      "metadata": {
        "id": "i8SsQr4L9hdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blob1_storage(spark, bucket_name, nombre_archivo, credentials_path):\n",
        "    # Obtener el bucket\n",
        "    bucket = get_gcs_bucket(bucket_name, credentials_path)\n",
        "\n",
        "    blob = bucket.blob(nombre_archivo)\n",
        "\n",
        "    # Descargar el contenido del archivo como bytes\n",
        "    contenido_bytes = blob.download_as_bytes()\n",
        "    contenido_str = contenido_bytes.decode('utf-8')\n",
        "\n",
        "    # Crear un RDD a partir de las líneas del archivo\n",
        "    rdd = spark.sparkContext.parallelize(contenido_str.splitlines())\n",
        "\n",
        "    # Crear un DataFrame a partir del RDD\n",
        "    schema = StructType([StructField(\"data\", StringType(), True)])\n",
        "    df = spark.createDataFrame(rdd.map(lambda x: (x,)), schema)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "dA5HdiGv9vjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = files_list_from_folder(bucket_name, folder_prefix)"
      ],
      "metadata": {
        "id": "7ZVct6kc-w74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df_pyspark = blob_pac(bucket_name, credentials_path, file_list, prefix)"
      ],
      "metadata": {
        "id": "eqRFeSO6-y9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = combined_df_pyspark.toPandas()"
      ],
      "metadata": {
        "id": "NlWuWmQE-_Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "donation_id = pd.concat([generate_id(len(df)),df], axis = 1)\n",
        "load_df_to_bq(donation_id, project_id, dataset_id, 'your-table-id')\n",
        "execute_sql_script(project_id, blob1_prep )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkoMq38JA0N8",
        "outputId": "60e2814d-bd2a-44c6-8254-5e567d8acf2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation completed in BigQuery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Backup Blob 2**"
      ],
      "metadata": {
        "id": "5NVacMH7HB6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def blob2(bucket_name, credentials_path, file_list, folder_prefix):\n",
        "    # Inicializar SparkSession\n",
        "    spark = SparkSession.builder.appName(\"ProcesamientoGCS\").getOrCreate()\n",
        "\n",
        "    # Obtener el bucket\n",
        "    bucket = get_gcs_bucket(bucket_name, credentials_path)\n",
        "\n",
        "    # Fijar el prefijo del directorio\n",
        "    folder_prefix = folder_prefix\n",
        "\n",
        "    # Obtener iterador de blobs en el directorio 'folder_prefix'\n",
        "    blobs = bucket.list_blobs(prefix=folder_prefix)\n",
        "\n",
        "    # Filtrar solo los blobs correspondientes a archivos, no a subdirectorios\n",
        "    archivos_en_blob2 = [blob for blob in blobs if '/' not in blob.name[len(folder_prefix):] and blob.name.startswith(folder_prefix + 'your-file-pattern')]\n",
        "\n",
        "    # Verificar si hay archivos en el storage que no estén en la lista\n",
        "    archivos_a_procesar = [blob for blob in archivos_en_blob2 if blob.name[len(folder_prefix):] not in file_list]\n",
        "\n",
        "    # Si no hay archivos para procesar, devuelve None\n",
        "    if not archivos_a_procesar:\n",
        "        return None\n",
        "\n",
        "    # Procesar cada archivo en paralelo\n",
        "    combined_df = None\n",
        "\n",
        "    for blob in archivos_a_procesar:\n",
        "        nombre_archivo = blob.name[len(folder_prefix):]\n",
        "\n",
        "        # Imprimir el nombre del archivo que se está procesando\n",
        "        print(f\"Procesando archivo: {nombre_archivo}\")\n",
        "\n",
        "        # Procesar el archivo y obtener el DataFrame\n",
        "        df = storage_bf(spark, bucket_name, blob.name, credentials_path)\n",
        "\n",
        "        # Agregar una columna con el nombre del archivo\n",
        "        df = df.withColumn(\"filename\", lit(nombre_archivo))\n",
        "\n",
        "        # Unir el DataFrame actual con el DataFrame acumulado\n",
        "        if combined_df is None:\n",
        "            combined_df = df\n",
        "        else:\n",
        "            combined_df = combined_df.union(df)\n",
        "\n",
        "    return combined_df"
      ],
      "metadata": {
        "id": "KWQH1XLmG9zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_blob2 = \"\"\"\n",
        "CREATE OR REPLACE TABLE `your-project_id.your-dataset_id.your_table-id` AS\n",
        "WITH PrepStep1 AS (\n",
        "  SELECT donation_id,\n",
        "         filename,\n",
        "         data,\n",
        "         CASE WHEN SAFE_CAST(SUBSTR(CAST(TRIM(data) AS STRING), 1, 3) AS INT64) = 100 THEN SUBSTR(CAST(TRIM(data) AS STRING), 4, 8)\n",
        "              ELSE SUBSTR(CAST(TRIM(data) AS STRING), 3, 9) END id,\n",
        "         CASE WHEN SUBSTR(data, -3) LIKE \"%R%\" THEN RIGHT(data,3) ELSE NULL END transactional_details,\n",
        "         REGEXP_EXTRACT(data, r'(.{11})Ban') account_number,\n",
        "         REGEXP_REPLACE(SUBSTR(data, 84,12),r'^0*', '') amount,\n",
        "         CASE WHEN CAST(SUBSTR(data, 1, 3) AS INT64) > 100 THEN TRIM(SUBSTR(data, 12, STRPOS(SUBSTR(data, 12), '0') - 1))\n",
        "              ELSE TRIM(SUBSTR(data, 12, STRPOS(SUBSTR(data, 12), '0') - 1)) END AS names,\n",
        "         DATE(CONCAT(SUBSTR(filename,12,4),'-',SUBSTR(filename,10,2),'-',SUBSTR(filename,8,2))) donation_date\n",
        "FROM `your-project_id.your-dataset_id.your_table-id`\n",
        "),PrepStep2 AS (\n",
        "  SELECT donation_id,\n",
        "        filename,\n",
        "        data,\n",
        "        id,\n",
        "        account_number,\n",
        "        names,\n",
        "        amount,\n",
        "        transactional_details,\n",
        "        donation_date\n",
        " FROM PrepStep1\n",
        "),PrepStep3 AS (\n",
        "SELECT donation_id,\n",
        "        SUBSTR(filename,1,15) filename,\n",
        "        id,\n",
        "        account_number,\n",
        "        names,\n",
        "        transactional_details,\n",
        "        amount,\n",
        "        donation_date,\n",
        "        SPLIT(filename, '_')[OFFSET(3)] logs,\n",
        "        CASE WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 1 THEN 'Enero'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 2 THEN 'Febrero'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 3 THEN 'Marzo'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 4 THEN 'Abril'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 5 THEN 'Mayo'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 6 THEN 'Junio'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 7 THEN 'Julio'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 8 THEN 'Agosto'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 9 THEN 'Septiembre'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 10 THEN 'Octubre'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 11 THEN 'Noviembre'\n",
        "             WHEN EXTRACT(MONTH FROM DATE(donation_date)) = 12 THEN 'Diciembre' END month\n",
        "FROM PrepStep2\n",
        ")SELECT donation_id,\n",
        "        filename,\n",
        "        id,\n",
        "        account_number,\n",
        "        names,\n",
        "        transactional_details,\n",
        "        amount,\n",
        "        donation_date,\n",
        "        CONCAT('XXXX',month,' ',EXTRACT( YEAR FROM DATE(donation_date))) transaction_source,\n",
        "        3 transaction_source_code,\n",
        "        logs\n",
        "  FROM PrepStep3\n",
        ";\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "lOh9_n_5QFMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def storage_blob2(spark, bucket_name, nombre_archivo, credentials_path):\n",
        "    # Inicializar el cliente de almacenamiento con credenciales desde el archivo\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "        credentials_path,\n",
        "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
        "    )\n",
        "    client = storage.Client(credentials=credentials)\n",
        "\n",
        "    # Obtener el bucket\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "    blob = bucket.blob(nombre_archivo)\n",
        "\n",
        "    # Descargar el contenido del archivo como bytes\n",
        "    contenido_bytes = blob.download_as_bytes()\n",
        "\n",
        "    # Decodificar el contenido utilizando la codificación detectada (puedes ajustar la codificación según tus necesidades)\n",
        "    encoding = \"utf-8\"  # Ajusta la codificación según tu necesidad\n",
        "    contenido_str = contenido_bytes.decode(encoding)\n",
        "\n",
        "    # Dividir el contenido en líneas\n",
        "    lineas = contenido_str.splitlines()\n",
        "\n",
        "    # Eliminar la última línea si la lista de líneas no está vacía\n",
        "    if lineas:\n",
        "        lineas.pop()\n",
        "\n",
        "    # Construir el DataFrame\n",
        "    df = spark.createDataFrame(lineas, \"string\").toDF(\"data\")\n",
        "\n",
        "    # Agregar una columna con el nombre del archivo\n",
        "    df = df.withColumn(\"filename\", lit(nombre_archivo))\n",
        "\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "1__MQdAsLcJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list_blob2 = files_list_from_folder(bucket_name, 'folder_prefix/')"
      ],
      "metadata": {
        "id": "6eHHHJzxH9ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_pyspark = blob2(bucket_name, credentials_path, file_list_bf, 'folder_prefix/')"
      ],
      "metadata": {
        "id": "tbtypo4TH2lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = bf_df_pyspark.toPandas()"
      ],
      "metadata": {
        "id": "6Y4Qa_oFI-DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DonationId_generate = generate_id(len(df2))\n",
        "df2_prep = pd.concat([DonationId_generate, df2], axis=1)"
      ],
      "metadata": {
        "id": "_wqcyw_KKlX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_df_to_bq(df2_prep, project_id, dataset_id, 'your-table-id')\n",
        "execute_sql_script(project_id, prep_blob2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhH32Za8P3nS",
        "outputId": "8c13c412-afc2-4f94-bac6-f04920866c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation completed in BigQuery\n"
          ]
        }
      ]
    }
  ]
}